{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 0: Setup\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/Chapter00.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-00} -->\n",
    "\n",
    "Let's install the required packages and check our setup for this course.\n",
    "\n",
    "### üéâ Free Cohere API key\n",
    "\n",
    "Before you run this colab notebook, head over to this [link to redeem a free Cohere API key](https://docs.google.com/forms/d/e/1FAIpQLSc9x4nV8_nSQvJnaINO1j9NIa2IUbAJqrKeSllNNCCbMFmCxw/viewform?usp=sf_link).\n",
    "\n",
    "Alternatively if you have a Cohere API key feel free to proceed. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Weave\n",
    "\n",
    "\n",
    "The code cell below will prompt you to put in a W&B API key. You can get your API key by heading over to https://wandb.ai/authorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/elshehawy/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are different credentials in the netrc file and the environment. Using the environment value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: elshehawy.\n",
      "View Weave data at https://wandb.ai/elshehawy/rag-course/weave\n"
     ]
    }
   ],
   "source": [
    "# import weave\n",
    "import weave\n",
    "\n",
    "# initialize weave client\n",
    "weave_client = weave.init(\"rag-course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Cohere\n",
    "\n",
    "The code cell below will prompt you to put in a Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "cohere_client = cohere.ClientV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple-turn chat with Cohere's command-r-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/elshehawy/rag-course/r/call/01928681-4642-73f0-b2cf-fd7e6cdcd922\n"
     ]
    }
   ],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is retrieval augmented generation (RAG)?\"}\n",
    "    ],\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's head over to the weave URL to check out the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='09115901-316b-4b92-a330-a6f99f1f4248' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Retrieval-Augmented Generation (RAG) is a large language model that combines information retrieval and text generation capabilities. It is designed to generate responses or text based on retrieved relevant information from a given context or a provided set of documents.\\n\\nHere\\'s a more detailed explanation of RAG:\\n1. **Information Retrieval**: RAG has access to a large corpus of text documents, often referred to as a \"retrieval corpus\" or \"knowledge source.\" This corpus can be comprised of web pages, books, articles, or any textual content that serves as a source of information. When presented with a query or a context, RAG first retrieves relevant documents from this corpus.\\n2. **Text Generation**: After retrieving relevant documents, RAG generates a response or text based on the information it has retrieved. It combines the extracted knowledge with its language generation capabilities to produce coherent and informative output. This generated text can be in the form of answers to questions, summaries, continuations of text, or any other form of language generation.\\n3. **Integration**: The key aspect of RAG is how it integrates information retrieval and text generation. It doesn\\'t just retrieve and present the relevant documents; instead, it uses the retrieved information as context for its language generation process. This means that the generated text is influenced and informed by the content of the retrieved documents, resulting in more accurate and relevant responses.\\n4. **Training**: RAG models are typically trained on large-scale datasets that involve question-answering or text generation tasks. During training, the model learns to identify relevant information from the retrieval corpus and generate coherent and contextually appropriate responses.\\n5. **Applications**: RAG has various applications, including question answering, dialogue systems, text summarization, content generation, and more. It can be used in chatbots, virtual assistants, language-based search engines, or any system that requires generating text based on specific knowledge or context.\\n\\nThe advantage of RAG models is their ability to ground their responses in factual information, reducing the likelihood of generating text that is inaccurate or inconsistent with the provided context. This makes them particularly useful in scenarios where factual accuracy and informativeness are crucial, such as in knowledge-intensive tasks or applications that require reliable and trustworthy responses.')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=9.0, output_tokens=463.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=207.0, output_tokens=463.0))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ragpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
