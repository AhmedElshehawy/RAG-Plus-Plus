{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 0: Setup\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/Chapter00.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-00} -->\n",
    "\n",
    "Let's install the required packages and check our setup for this course.\n",
    "\n",
    "### ðŸŽ‰ Free Cohere API key\n",
    "\n",
    "Before you run this colab notebook, head over to this [link to redeem a free Cohere API key](https://docs.google.com/forms/d/e/1FAIpQLSc9x4nV8_nSQvJnaINO1j9NIa2IUbAJqrKeSllNNCCbMFmCxw/viewform?usp=sf_link).\n",
    "\n",
    "Alternatively if you have a Cohere API key feel free to proceed. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Weave\n",
    "\n",
    "\n",
    "The code cell below will prompt you to put in a W&B API key. You can get your API key by heading over to https://wandb.ai/authorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weave\n",
    "import weave\n",
    "\n",
    "# initialize weave client\n",
    "weave_client = weave.init(\"rag-course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Cohere\n",
    "\n",
    "The code cell below will prompt you to put in a Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "cohere_client = cohere.ClientV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple-turn chat with Cohere's command-r-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is retrieval augmented generation (RAG)?\"}\n",
    "    ],\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's head over to the weave URL to check out the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='63ecdeec-9b8b-4c30-be74-0a9b25ac2b7f' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text=\"Retrieval-Augmented Generation (RAG) is a natural language processing (NLP) technique that combines information retrieval (retrieving relevant documents or passages from a large corpus) with language generation (generating a response based on the retrieved information).\\n\\nHere's a more detailed explanation:\\n\\n1. **Information Retrieval**: RAG models are provided with access to a large corpus of text, such as Wikipedia or a collection of documents. When given an input (such as a question or a prompt), the model first retrieves relevant documents or passages from this corpus. This retrieval step ensures that the model has access to factual information related to the input.\\n\\n2. **Language Generation**: After retrieving relevant information, the RAG model uses a language generator to create a response. This generator can be a language model like GPT (Generative Pre-trained Transformer), which has been trained on a large corpus of text to predict the next word in a sentence. The generator takes into account both the input and the retrieved information to generate a coherent and contextually appropriate response.\\n\\nThe key advantage of RAG is that it combines the strengths of information retrieval systems (which can provide factual and accurate information) with the flexibility of language generation models (which can create coherent and contextually appropriate text). By grounding the language generation process in retrieved facts, RAG models aim to improve the accuracy and faithfulness of the generated responses.\\n\\nRAG models have various applications, including question answering, dialogue systems, text generation, and data-to-text generation (generating text from structured data). They have shown promising results in generating more accurate and informative responses compared to language generation models that do not have an information retrieval component.\")], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=9.0, output_tokens=341.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=207.0, output_tokens=341.0))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
